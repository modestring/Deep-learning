# 总结一下 PCA 的算法步骤：

设有 m 条 n 维数据。

1.将原始数据按列组成 n 行 m 列矩阵 X；

2.将 X 的每一行进行零均值化，即减去这一行的均值；

3.求出协方差矩阵 C=1/m XX(T)；

4.求出协方差矩阵的特征值及对应的特征向量；

5.将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；

6.Y=PX 即为降维到 k 维后的数据。



## 性质
1.缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；

2.降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；

3.过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；

4.特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；


## 细节
1 零均值化
当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。而对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来，不能使用验证集或者测试集的中心向量。

其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。

另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。

2 与 SVD 的对比
这是两个不同的数学定义。结论：特征值和特征向量是针对方阵才有的，而对任意形状的矩阵都可以做奇异值分解。



## PCA 问题可以转换成 SVD 求解。而实际上 Sklearn 的 PCA 就是用 SVD 进行求解的，原因有以下几点：

当样本维度很高时，协方差矩阵计算太慢；
方阵特征值分解计算效率不高；
SVD 除了特征值分解这种求解方式外，还有更高效更准球的迭代求解方式，避免了A(T)A 的计算；
其实 PCA 与 SVD 的右奇异向量的压缩效果相同。
